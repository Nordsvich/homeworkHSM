{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP HW3: Movie Review Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Alexander Marinskiy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "1. Choose and argue your measure of a test's accuracy.\n",
    "2. Build data processing and classification pipeline; Please compare word-embeddings vs classical methods \n",
    "3. Tune your model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Embedding, LSTM\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train = pd.read_csv('dataset/train.tsv', sep=\"\\t\")\n",
    "test = pd.read_csv('dataset/test.tsv', sep=\"\\t\")\n",
    "sub = pd.read_csv('dataset/sampleSubmission.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "5         6           1  of escapades demonstrating the adage that what...   \n",
       "6         7           1                                                 of   \n",
       "7         8           1  escapades demonstrating the adage that what is...   \n",
       "8         9           1                                          escapades   \n",
       "9        10           1  demonstrating the adage that what is good for ...   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  \n",
       "5          2  \n",
       "6          2  \n",
       "7          2  \n",
       "8          2  \n",
       "9          2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check what dataset looks like\n",
    "train.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 156060 rows and 4 columns in our dataset\n"
     ]
    }
   ],
   "source": [
    "print('There are', train.shape[0], 'rows and', train.shape[1], 'columns in our dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    79582\n",
       "3    32927\n",
       "1    27273\n",
       "4     9206\n",
       "0     7072\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the distribution of labels in dataset\n",
    "train[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing the distribution\n",
    "ax = sns.barplot(x=\"Sentiment\", \n",
    "                 y=\"Sentiment\", \n",
    "                 data=train, \n",
    "                 estimator=lambda x: len(x) / len(train) * 100, palette=\"GnBu_d\", \n",
    "                 orient='h')\n",
    "ax.set(xlabel=\"Percent\", title='Distribution of classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment labels are:\n",
    "\n",
    "- 0 - negative\n",
    "- 1 - somewhat negative\n",
    "- 2 - neutral\n",
    "- 3 - somewhat positive\n",
    "- 4 - positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that more than 50% of the phrases in the dataset are neutral. Also, the number of somewhat positive phrases is approximately equal to the number of somewhat negative ones, and the number of positive ones is approximately equal to the number of negative ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choosing metric for quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kaggle competition is evaluated on classification accuracy (the percent of labels that are predicted correctly). Since this is how the final result will be defined, there are no real reason to use different metric for quality. Therefore, I will use accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary with names of vectorizers and vectorizers\n",
    "vecrotizers={'Tf-idf vectorizer': TfidfVectorizer(sublinear_tf=True, \n",
    "                                                  min_df=5, \n",
    "                                                  norm='l2', \n",
    "                                                  encoding='latin-1', \n",
    "                                                  ngram_range=(1, 2), \n",
    "                                                  stop_words='english'),\n",
    "             'Count vectorizer': CountVectorizer(min_df=5, \n",
    "                                                 encoding='latin-1', \n",
    "                                                 stop_words='english', \n",
    "                                                 ngram_range=(1, 2))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with models\n",
    "models = {'Multinomial Naive Bayes': MultinomialNB(),\n",
    "          'Logistic Regression': LogisticRegression(random_state=42)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get scores for specific vectorizer and model. \n",
    "def evaluate_model(X, y, vectorizer_name, vectorizer, model_name, model):\n",
    "    # create pipline to fit vectorizer on each fold\n",
    "    clf = make_pipeline(vectorizer, model)\n",
    "    \n",
    "    # get scores\n",
    "    scores = cross_validate(clf, \n",
    "                            X, \n",
    "                            y, \n",
    "                            scoring=['accuracy'], \n",
    "                            cv=5)\n",
    "    \n",
    "    # print results\n",
    "    print('Accuracy for', model_name, 'model with', vectorizer_name, 'on cross-validation is', np.mean(scores['test_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Multinomial Naive Bayes model with Tf-idf vectorizer on cross-validation is 0.5399076228786248\n",
      "Accuracy for Multinomial Naive Bayes model with Count vectorizer on cross-validation is 0.5523515243763759\n",
      "Accuracy for Logistic Regression model with Tf-idf vectorizer on cross-validation is 0.5778738303431306\n",
      "Accuracy for Logistic Regression model with Count vectorizer on cross-validation is 0.5760732748979114\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    for vectorizer_name, vectorizer in vecrotizers.items():\n",
    "        evaluate_model(train.Phrase, train.Sentiment, vectorizer_name, vectorizer, model_name, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Embedings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For word embeddings approach I'm going to use pre-trained GloVe model (GloVe 6B): https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = train.Phrase\n",
    "\n",
    "# define class labels\n",
    "labels = pd.get_dummies(train.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of vocabulary is 15289\n"
     ]
    }
   ],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print('The size of vocabulary is', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of encoded document:\n",
      "[2, 323, 3, 14150, 6028, 1, 6586, 9, 52, 8, 46, 13, 1, 2976, 8, 177, 46, 13, 1, 10913, 65, 3, 78, 668, 10117, 19, 576, 3, 78, 2123, 5, 57, 3, 2, 42]\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print('Example of encoded document:')\n",
    "print(encoded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of padded document:\n",
      "[    2   323     3 14150  6028     1  6586     9    52     8    46    13\n",
      "     1  2976     8   177    46    13     1 10913    65     3    78   668\n",
      " 10117    19   576     3    78  2123     5    57     3     2    42     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0]\n"
     ]
    }
   ],
   "source": [
    "# pad documents \n",
    "max_length = max(len(doc) for doc in encoded_docs)\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print('Example of padded document:')\n",
    "print(padded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets prepare training and test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple 3 layers neural network with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alex\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Alex\\Anaconda2\\envs\\Python3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 49, 100)           1528900   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4900)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              5018624   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 7,602,249\n",
      "Trainable params: 6,073,349\n",
      "Non-trainable params: 1,528,900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set checkpoint\n",
    "filepath=\"simple-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          epochs=50, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alex\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "7803/7803 [==============================] - 4s 537us/step\n",
      "Accuracy simple 3 layer neural network: 61.245675\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('simple-45-0.61.hdf5')\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy simple 3 layer neural network: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm=Sequential()\n",
    "model_lstm.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "model_lstm.add(LSTM(64,dropout=0.4, recurrent_dropout=0.4,return_sequences=True))\n",
    "model_lstm.add(LSTM(32,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
    "model_lstm.add(Dense(5,activation='softmax'))\n",
    "model_lstm.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set checkpoint\n",
    "filepath=\"lstm-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.fit(X_train, \n",
    "          y_train, \n",
    "          epochs=50, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7803/7803 [==============================] - 7s 941us/step\n",
      "Accuracy of LSTM model: 62.386262\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model_lstm = load_model('lstm-06-0.62.hdf5')\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model_lstm.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy of LSTM model: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I stopped the training early because the deadline was close. It seems that LSTM can show even better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that models based on Word Embeddings show noticeably better quality on this task.\n",
    "\n",
    "* Out of the classical models, Logic regression with tf-idf vectorizer showed the best quality. \n",
    "* Out of the depth learning models, the LSTM model had the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
