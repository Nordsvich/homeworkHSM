{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework №1: Spam detection \n",
    "*Bezhenaer OLga*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content of the task:\n",
    "1. Download sms-spam dataset https://archive.ics.uci.edu/ml/ datasets/sms+spam+collection\n",
    "2. Choose and argument metric for quality\n",
    "3. Code «by a hands» naive bayes for spam detection task;\n",
    "4. Choose a measure of a test's accuracy and argument your choice; Perform 5-fold validation for this task;\n",
    "5. Compare your results with sklearn naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Коля\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Коля\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "import re \n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk  \n",
    "nltk.download(\"punkt\")  \n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data loading \n",
    "data = pd.read_csv('SMSSpamCollection', sep='\\t', names=['label', 'message'])\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory data analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"].value_counts() #the problem of sample imbalance is presented "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change in labels:spam- positive class \"1\", \"ham\" - negaive class \"0\"\n",
    "data['label'] = data['label'].map({'ham': 0, 'spam': 1})\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to the lower case \n",
    "data['message'] = data['message'].map(lambda x: x.lower())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing punctuation\n",
    "data['message'] = data.message.str.replace('[^\\w\\s]', '')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing digits\n",
    "data['message'] = data.message.str.replace('[0-9]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the text into single words\n",
    "data['message'] = data.message.apply(nltk.word_tokenize)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implemention of the word stemmimg\n",
    "stemmer = PorterStemmer()\n",
    "data['message'] = data['message'].apply(lambda x: [stemmer.stem(y) for y in x])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [go, until, jurong, point, crazi, avail, onli,...\n",
       "1                         [ok, lar, joke, wif, u, oni]\n",
       "2    [free, entri, in, a, wkli, comp, to, win, fa, ...\n",
       "3    [u, dun, say, so, earli, hor, u, c, alreadi, t...\n",
       "4    [nah, i, dont, think, he, goe, to, usf, he, li...\n",
       "5    [freemsg, hey, there, darl, it, been, week, no...\n",
       "6    [even, my, brother, is, not, like, to, speak, ...\n",
       "7    [as, per, your, request, mell, mell, oru, minn...\n",
       "8    [winner, as, a, valu, network, custom, you, ha...\n",
       "9    [had, your, mobil, month, or, more, u, r, enti...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the results\n",
    "data['message'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['message'] = data['message'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the total set: 5572\n",
      "Number of rows in the training set: 4179\n",
      "Number of rows in the test set: 1393\n"
     ]
    }
   ],
   "source": [
    "# Creating training and testing sets.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['message'],data['label'],random_state=42)\n",
    "\n",
    "print('Number of rows in the total set: {}'.format(data.shape[0]))\n",
    "print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n",
    "print('Number of rows in the test set: {}'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data trasformation into occurence that becomes features in the model (also here we remove stop words)\n",
    "count_vect = CountVectorizer(stop_words='english')  \n",
    "X_train_new = count_vect.fit_transform(X_train)  \n",
    "\n",
    "X_test_new = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Metric for quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was mentioned above, the problem of sample imbalance is presented in our data. In that case the \"accuracy\" score does not show trustworthy results. The main metrics for quality might be recalls, precision and f1-score. What is important for us? \n",
    " - Equiality of recall_1 and recall_0 (meaning that classifier predicts well both positive and negative classes) the close recall_1 and recall_0 too each other - the better quality of the classifier;\n",
    " - F1-score - it comprises both precision and recall metrics and does not suffer as \"accuracy\" metric from sample imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Naive bayes \"by hands\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes' theorem: P(A | B) = P(B | A) * P(A) / P(B)\n",
    "P(A | B) - Probability that a text containing a given word is spam ( P(Spam|Word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to find these 4 probabilities:\n",
    "- Overall probability that any given message is spam: P(spam)\n",
    "- Overall probability that any given message is not spam (is ham): P(ham)\n",
    "- Probability that a word appears in spam messages: P(word/spam)\n",
    "- Probability that a word appears in ham messages: P(word/ham)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step. 1 Finding overall probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of hams and spams\n",
    "number_of_hams = y_train.value_counts()[0]\n",
    "number_of_spams=y_train.value_counts()[1]\n",
    "total = y_train.value_counts()[0]+y_train.value_counts()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(ham) = number of documents belonging to category ham / total number of documents \n",
    "\n",
    "P(spam) = nnumder of documents belonging to category spam / total numder of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of spam message 0.134\n",
      "Probability of usual message 0.866\n"
     ]
    }
   ],
   "source": [
    "#Finding overall probabalities\n",
    "p_spam= number_of_spams/total #P(spam)\n",
    "p_ham= number_of_hams/total #P(ham)\n",
    "\n",
    "print(\"Probability of spam message\",p_spam.round(3) )\n",
    "print(\"Probability of usual message\",p_ham.round(3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step. 2 Finding probability that a word appears in spam/ham messages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(word1 | ham) = (count of word1 belonging to category ham + 1) / (total count of words belonging to ham + number of distinct words in training data sets i.e. our database)\n",
    "\n",
    "P(word1 | spam) = (count of word1 belonging to category spam + 1) / (total count of words belonging to spam + number of distinct words in training data sets i.e. our database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: I honesty failed in the implementation of the Step 2., and really do not understand how to do it \"by hands\". That is why the part of code with Step 2, I copy/paste from the work of my colleague,Alexander Marinskiy. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find probabilities for spam\n",
    "indices = np.where(y_train == 1)[0]\n",
    "spam = X_train_new.tocsr()[indices,:]\n",
    "\n",
    "frequency_spam = spam.toarray().sum(axis=0) + 1\n",
    "probability_spam = frequency_spam / (sum(frequency_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find probabilities for ham\n",
    "indices = np.where(y_train == 0)[0]\n",
    "ham = X_train_new.tocsr()[indices,:]\n",
    "\n",
    "frequency_ham = ham.toarray().sum(axis=0) + 1\n",
    "probability_ham = frequency_ham / (sum(frequency_ham))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log(P(ham | bodyText)) = log(P(ham)) + log(P(bodyText | ham))\n",
    "= log(P(ham)) + log(P(word1 | ham)) + log(P(word2 | ham)) …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_or_ham(arr):\n",
    "    prob_ham = np.log(p_ham)\n",
    "    prob_spam = np.log(p_spam)\n",
    "    arr = scipy.sparse.find(arr)\n",
    "    for i in range(len(arr[1])):\n",
    "        prob_ham = prob_ham + np.log(probability_ham[arr[1][i]]) * arr[2][i]\n",
    "        prob_spam = prob_spam + np.log(probability_spam[arr[1][i]]) * arr[2][i]\n",
    "\n",
    "    if prob_ham >= prob_spam:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "ans = []\n",
    "for i in X_test_new:\n",
    "    ans.append(spam_or_ham(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Checking the model quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      1207\n",
      "          1       0.95      0.92      0.93       186\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average high f1-score, but if we look at the recalls: they are sufficiently different. Model works in favor of the majority class ( 0- \"ham\"). It can be caused by sample imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sklearn naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "model = MultinomialNB().fit(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the model\n",
    "predictions = model.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      1207\n",
      "          1       0.95      0.92      0.93       186\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 5-fold cross validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the results of \"by hand\" and sklearn naive_bayes looks quite similar. 5 fols cross-validation was implemented for the sklearn model. First cross-validation is done for the \"accuracy\" and than for the \"f1\" score. In oder to compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores:  [0.97729988 0.96172249 0.96052632 0.96407186 0.97245509]\n",
      "Average Accuracy score:  0.9672151260922446\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(estimator=model, X=X_train_new, y=y_train, cv=5, scoring='accuracy')\n",
    "print('Accuracy scores: ', scores['test_score'])\n",
    "print('Average Accuracy score: ', np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a high metric value, but! when we look at \"F1\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores:  [0.91914894 0.86554622 0.85957447 0.86956522 0.89956332]\n",
      "Average F1 score:  0.8826796317822622\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(estimator=model, X=X_train_new, y=y_train, cv=5, scoring='f1')\n",
    "print('F1 scores: ', scores['test_score'])\n",
    "print('Average F1 score: ', np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that average F1 score is sufficient lower that accuracy.This means that model does not work as well as we would like, even despite the high \"accuracy\" rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "The quality of the \"by hand\" model and sklearn model looks the same (but this is due to \"good job done\" by my collegue) and even models performs quiet qood results (f1 = 0.99 - negativ class,f1=0.93-positiv) The difference between this two f1 scores confirms that  the problem of sample impalance spoils the results in favor of the majority class(ham or negetiv class). Also cross-validation confirms the nessecity of solving sample imbalance problem (may be this can be done by dowmsampling the majority class) or also better data preprocessing is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
