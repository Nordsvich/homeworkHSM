{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anton Anisimov NLP HM â„–3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# data prep\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from gensim.models import word2vec\n",
    "# models\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set balance\n",
      "2    79582\n",
      "3    32927\n",
      "1    27273\n",
      "4     9206\n",
      "0     7072\n",
      "Name: Sentiment, dtype: int64\n",
      "Test set balance\n",
      "2    66292\n",
      "Name: Sentiment, dtype: int64\n",
      "(222352, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhraseId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Phrase  Sentiment\n",
       "PhraseId                                                              \n",
       "1         A series of escapades demonstrating the adage ...          1\n",
       "2         A series of escapades demonstrating the adage ...          2\n",
       "3                                                  A series          2\n",
       "4                                                         A          2\n",
       "5                                                    series          2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tran set\n",
    "df_train=pd.read_csv(\"train.tsv\", sep=\"\\t\",quoting=3)\n",
    "print ('Train set balance')\n",
    "print (df_train['Sentiment'].value_counts())\n",
    "# load test set\n",
    "df_test=pd.read_csv(\"test.tsv\", sep=\"\\t\",quoting=3)\n",
    "df_y=pd.read_csv(\"sampleSubmission.csv\")\n",
    "df_test['Sentiment']=df_y['Sentiment']\n",
    "print ('Test set balance')\n",
    "print (df_test['Sentiment'].value_counts())\n",
    "# Merge sets\n",
    "a=df_train['Sentiment'].tolist()+df_test['Sentiment'].tolist()\n",
    "df=pd.concat([df_train, df_test], axis=0,sort=False)\n",
    "df.index= df['PhraseId']\n",
    "df=df.drop(columns=['PhraseId','SentenceId'])\n",
    "df['Sentiment']=a\n",
    "print (df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above test set consists of class 2.So for testing perfomance I decided to choose f1 weighted metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline:</br>\n",
    "    1)Merge sets to create dictionaries\n",
    "    2)Create 3 datasets(bow,tfidf,w2v)\n",
    "    3)Split to train and test\n",
    "    4)Train and test two models (log-reg and random forest)\n",
    "    6)Try to tune and balance\n",
    "    7)Compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember the last index of train set.After clean merged set and create bag and ifidf. Finally split to train and test by rembered indexes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ind=df_train['PhraseId'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=df['Sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary cleaning\n",
    "def review_to_words( raw_review ):\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "     # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    return( \" \".join( meaningful_words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\ProgramData\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "# delete trash1 \n",
    "num_reviews = df[\"Phrase\"].size\n",
    "clean_train_reviews = []\n",
    "for i in range(0, num_reviews):\n",
    "    clean_train_reviews.append( review_to_words( df.iloc[i,0] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1871c7a90483>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#reconstract table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_train_reviews\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Obs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#reconstract table\n",
    "a=df.index\n",
    "df=pd.DataFrame(clean_train_reviews,columns=['Obs'])\n",
    "df['features']=labels\n",
    "df.index=a\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete empty raws\n",
    "df.replace('', np.nan, inplace=True)\n",
    "df.dropna(subset=['Obs'], inplace=True)\n",
    "# delete duplicates\n",
    "df.drop_duplicates(subset ='Obs', keep = False, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw data set=156060,\n",
    "new preclean data =40448\n",
    "special ind=156060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data_merge_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load check point data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ind=156060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data function just for convince\n",
    "def load_set(name):\n",
    "    df=pd.read_csv(name)\n",
    "    df.index=df.iloc[:,0]\n",
    "    df=df.drop(columns=df.columns[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Obs</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhraseId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>series escapades demonstrating adage good goos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>series escapades demonstrating adage good goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>demonstrating adage good goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>demonstrating adage</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        Obs  features\n",
       "PhraseId                                                             \n",
       "1         series escapades demonstrating adage good goos...         1\n",
       "2           series escapades demonstrating adage good goose         2\n",
       "9                                                 escapades         2\n",
       "10                           demonstrating adage good goose         2\n",
       "11                                      demonstrating adage         2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=load_set('data_merge_clean.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    33145\n",
       "3     7750\n",
       "1     6670\n",
       "4     2120\n",
       "0     1620\n",
       "Name: features, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['features'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ð¡lasses are unbalased, so<br/> 1)ignore<br/> 2)naive cut <br/> 3)custom penalty in lost function<br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51305, 17188)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bags transform\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None) \n",
    "train_data_features = vectorizer.fit_transform(df['Obs'],df.iloc[:,-1])\n",
    "colums=vectorizer.get_feature_names()\n",
    "train_data_features = train_data_features.toarray()\n",
    "df_bags=pd.DataFrame(train_data_features,columns=colums,index=df.index)\n",
    "df_bags['f_labels']=df.iloc[:,-1].tolist()\n",
    "df_bags.index=df.index\n",
    "# df_bags.to_csv('bow.csv')\n",
    "df_bags.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becuse of memory leak I will use models consistently:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-reg BOW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_log=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my log for conv\n",
    "def my_log(X_train,X_test,y_train,y_test):\n",
    "    Model = LogisticRegression(random_state=0, solver='newton-cg',\n",
    "                           multi_class='multinomial',penalty='l2').fit(X_train, y_train)\n",
    "    y_pred=Model.predict(X_train)\n",
    "    print (metrics.classification_report(y_train,y_pred))\n",
    "    train_scores=metrics.f1_score(y_train,y_pred,average='macro')\n",
    "    train_scores_1=metrics.f1_score(y_train,y_pred,average='weighted')\n",
    "    y_pred=Model.predict(X_test)\n",
    "    test_scores=metrics.f1_score(y_test,y_pred,average='weighted')\n",
    "    print ('train_macro:',train_scores,'train_weighted:',train_scores_1,'test:',test_scores)\n",
    "    return Model,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split\n",
    "index_train=df.index<=split_ind\n",
    "a=(index_train+0).sum()\n",
    "# bow split\n",
    "X_b_train, X_b_test =df_bags.iloc[:a],df_bags.iloc[a:]\n",
    "del df_bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.52      0.65      1620\n",
      "           1       0.79      0.58      0.67      6670\n",
      "           2       0.75      0.95      0.84     18924\n",
      "           3       0.79      0.63      0.70      7750\n",
      "           4       0.89      0.59      0.71      2120\n",
      "\n",
      "    accuracy                           0.77     37084\n",
      "   macro avg       0.82      0.65      0.71     37084\n",
      "weighted avg       0.78      0.77      0.76     37084\n",
      "\n",
      "train_macro: 0.7148329826287129 train_weighted: 0.764255165847601 test: 0.8128886848365959\n"
     ]
    }
   ],
   "source": [
    "# run log reg\n",
    "_,y_pred=my_log(X_b_train.iloc[:,:-1],X_b_test.iloc[:,:-1],X_b_train.iloc[:,-1],X_b_test.iloc[:,-1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      0.68      0.81     14221\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.68     14221\n",
      "   macro avg       0.20      0.14      0.16     14221\n",
      "weighted avg       1.00      0.68      0.81     14221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (metrics.classification_report(X_b_test.iloc[:,-1],y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So train score are bad, lets try to use stupid cut(to 2000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    18924\n",
       "3     7750\n",
       "1     6670\n",
       "4     2120\n",
       "0     1620\n",
       "Name: f_labels, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check balance\n",
    "X_b_train['f_labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance cut\n",
    "for i in range(1,5):\n",
    "    a=X_b_train['f_labels']==i\n",
    "    a=X_b_train.index[a]\n",
    "    shuffle_ap=np.random.permutation(len(a))\n",
    "    index=a[shuffle_ap[3000:]]\n",
    "    if i == 1:\n",
    "        X_b_train1=X_b_train.drop(index=index)\n",
    "    else:\n",
    "        X_b_train1=X_b_train1.drop(index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    3000\n",
       "2    3000\n",
       "1    3000\n",
       "4    2120\n",
       "0    1620\n",
       "Name: f_labels, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b_train1['f_labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.87      1620\n",
      "           1       0.86      0.89      0.88      3000\n",
      "           2       0.84      0.91      0.87      3000\n",
      "           3       0.87      0.86      0.86      3000\n",
      "           4       0.90      0.86      0.88      2120\n",
      "\n",
      "    accuracy                           0.87     12740\n",
      "   macro avg       0.88      0.87      0.87     12740\n",
      "weighted avg       0.87      0.87      0.87     12740\n",
      "\n",
      "train_macro: 0.8712383644151173 train_weighted: 0.8712833741630417 test: 0.6295653849860268\n"
     ]
    }
   ],
   "source": [
    "_,y_pred=my_log(X_b_train1.iloc[:,:-1],X_b_test.iloc[:,:-1],X_b_train1.iloc[:,-1],X_b_test.iloc[:,-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      0.46      0.63     14221\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.46     14221\n",
      "   macro avg       0.20      0.09      0.13     14221\n",
      "weighted avg       1.00      0.46      0.63     14221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (metrics.classification_report(X_b_test.iloc[:,-1],y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result:maybe it was a bad idea,but this model now provides with balanced prediction. Moreover on test set we lost 0.23 on recall, but precision still be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest BOW:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made several experiments with cv and grid serch and found some optimal parametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_rf(X_train,X_test,y_train,y_test,nt,deep):\n",
    "    Model = RandomForestClassifier(n_estimators=nt,max_depth=deep).fit(X_train, y_train)\n",
    "    y_pred=Model.predict(X_train)\n",
    "    train_scores=metrics.f1_score(y_train,y_pred,average='macro')\n",
    "    train_scores_1=metrics.f1_score(y_train,y_pred,average='weighted')\n",
    "    y_pred=Model.predict(X_test)\n",
    "    test_scores=metrics.f1_score(y_test,y_pred,average='weighted')\n",
    "    print ('train_macro:',train_scores,'train_weighted:',train_scores_1,'test:',test_scores)\n",
    "    return Model,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_macro: 0.3894973771800297 train_weighted: 0.5059661768980499 test: 0.9894475039971575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                        max_depth=70, max_features='auto', max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=150,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " array([2, 2, 2, ..., 2, 2, 2], dtype=int64))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rf(X_b_train.iloc[:,:-1],X_b_test.iloc[:,:-1],X_b_train.iloc[:,-1],X_b_test.iloc[:,-1],150,70)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51305, 17188)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "Vec=vectorizer.fit_transform(df['Obs'],df.iloc[:,-1])\n",
    "col=vectorizer.get_feature_names()\n",
    "df_tf=pd.DataFrame(Vec.todense(),columns=col)\n",
    "df_tf['f_labels']=df.iloc[:,-1].tolist()\n",
    "df_tf.index=df.index\n",
    "# df_tf.to_csv('tfidf.csv')\n",
    "df_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split\n",
    "index_train=df.index<=split_ind\n",
    "a=(index_train+0).sum()\n",
    "# bow split\n",
    "X_tf_train, X_tf_test =df_tf.iloc[:a],df_tf.iloc[a:]\n",
    "del df_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-reg tfidf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.16      0.26      1620\n",
      "           1       0.70      0.42      0.53      6670\n",
      "           2       0.68      0.94      0.79     18924\n",
      "           3       0.70      0.53      0.61      7750\n",
      "           4       0.83      0.27      0.41      2120\n",
      "\n",
      "    accuracy                           0.69     37084\n",
      "   macro avg       0.74      0.47      0.52     37084\n",
      "weighted avg       0.70      0.69      0.66     37084\n",
      "\n",
      "train_macro: 0.5194305465404697 train_weighted: 0.6601275296344653 test: 0.8674842717209524\n"
     ]
    }
   ],
   "source": [
    "_,y_pred=my_log(X_tf_train.iloc[:,:-1],X_tf_test.iloc[:,:-1],X_tf_train.iloc[:,-1],X_tf_test.iloc[:,-1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      0.77      0.87     14221\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.77     14221\n",
      "   macro avg       0.20      0.15      0.17     14221\n",
      "weighted avg       1.00      0.77      0.87     14221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (metrics.classification_report(X_tf_test.iloc[:,-1],y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF tfidf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_macro: 0.3745370902802025 train_weighted: 0.5009504217598435 test: 0.9907384407934424\n"
     ]
    }
   ],
   "source": [
    "_,y_pred=my_rf(X_tf_train.iloc[:,:-1],X_tf_test.iloc[:,:-1],X_tf_train.iloc[:,-1],X_tf_test.iloc[:,-1],250,70)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      0.98      0.99     14221\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98     14221\n",
      "   macro avg       0.20      0.20      0.20     14221\n",
      "weighted avg       1.00      0.98      0.99     14221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (metrics.classification_report(X_tf_test.iloc[:,-1],y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mean_vectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(next(iter(w2v.values())))\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51305, 301)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odel = word2vec.Word2Vec(df.iloc[:,0],size=300,window=3, workers=5)\n",
    "w2v = dict(zip(odel.wv.index2word, odel.wv.syn0))\n",
    "data_mean=mean_vectorizer(w2v).fit(df.iloc[:,0]).transform(df.iloc[:,0])\n",
    "df_w2v=pd.DataFrame(data=data_mean)\n",
    "df_w2v['labels']=df.iloc[:,-1].tolist()\n",
    "df_w2v.index=df.index\n",
    "# df_w2v.to_csv('w2v.csv')\n",
    "df_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split\n",
    "index_train=df.index<=split_ind\n",
    "a=(index_train+0).sum()\n",
    "# bow split\n",
    "X_w2v_train, X_w2v_test =df_w2v.iloc[:a],df_w2v.iloc[a:]\n",
    "del df_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1620\n",
      "           1       0.23      0.00      0.01      6670\n",
      "           2       0.51      0.99      0.67     18924\n",
      "           3       0.18      0.01      0.01      7750\n",
      "           4       0.00      0.00      0.00      2120\n",
      "\n",
      "    accuracy                           0.51     37084\n",
      "   macro avg       0.18      0.20      0.14     37084\n",
      "weighted avg       0.34      0.51      0.35     37084\n",
      "\n",
      "train_macro: 0.1390296067164321 train_weighted: 0.34777196196932597 test: 0.9930253142149054\n"
     ]
    }
   ],
   "source": [
    "_,y_pred=my_log(X_w2v_train.iloc[:,:-1],X_w2v_test.iloc[:,:-1],X_w2v_train.iloc[:,-1],X_w2v_test.iloc[:,-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      0.99      0.99     14221\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99     14221\n",
      "   macro avg       0.33      0.33      0.33     14221\n",
      "weighted avg       1.00      0.99      0.99     14221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (metrics.classification_report(X_w2v_test.iloc[:,-1],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance cut\n",
    "for i in range(1,4):\n",
    "    a=X_w2v_train['labels']==i\n",
    "    a=X_w2v_train.index[a]\n",
    "    shuffle_ap=np.random.permutation(len(a))\n",
    "    index=a[shuffle_ap[3000:]]\n",
    "    if i == 1:\n",
    "        X_w2v_train1=X_w2v_train.drop(index=index)\n",
    "    else:\n",
    "        X_w2v_train1=X_w2v_train1.drop(index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.01      0.02      1620\n",
      "           1       0.27      0.46      0.34      3000\n",
      "           2       0.39      0.48      0.43      3000\n",
      "           3       0.28      0.32      0.30      3000\n",
      "           4       0.23      0.05      0.08      2120\n",
      "\n",
      "    accuracy                           0.30     12740\n",
      "   macro avg       0.27      0.26      0.23     12740\n",
      "weighted avg       0.28      0.30      0.27     12740\n",
      "\n",
      "train_macro: 0.23259224833776493 train_weighted: 0.2665798073407488 test: 0.49878602343502587\n"
     ]
    }
   ],
   "source": [
    "_,y_pred=my_log(X_w2v_train1.iloc[:,:-1],X_w2v_test.iloc[:,:-1],X_w2v_train1.iloc[:,-1],X_w2v_test.iloc[:,-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      0.33      0.50     14221\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.33     14221\n",
      "   macro avg       0.20      0.07      0.10     14221\n",
      "weighted avg       1.00      0.33      0.50     14221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (metrics.classification_report(X_w2v_test.iloc[:,-1],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_macro: 0.9993182490348606 train_weighted: 0.9992179472696351 test: 0.9244441083043413\n"
     ]
    }
   ],
   "source": [
    "_,y_pred=my_rf(X_w2v_train.iloc[:,:-1],X_w2v_test.iloc[:,:-1],X_w2v_train.iloc[:,-1],X_w2v_test.iloc[:,-1],200,150)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       1.00      0.86      0.92     14221\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.86     14221\n",
      "   macro avg       0.25      0.21      0.23     14221\n",
      "weighted avg       1.00      0.86      0.92     14221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (metrics.classification_report(X_w2v_test.iloc[:,-1],y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ð¡NN aka stupid expirement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to implement CNN model with custom naive and stupid custom loss function to try to work with class unbalance. However, for this experiment I do not use basic test set, only old school( take random from hole set). As a metric I chosed f1 macro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 2 leyers CNN\n",
    "class mynet(nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    super(mynet, self).__init__()\n",
    "    self.linear1 = nn.Linear(D_in, H) \n",
    "    self.linear2 = nn.Linear(H, D_out)\n",
    "  def forward(self, x):\n",
    "    inputs=self.linear1(x)\n",
    "    act=nn.ReLU()\n",
    "    x=act(inputs)\n",
    "    x=self.linear2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For penaly for my stupid lost function, I will try to not use weights sum like L2 or L1, instead I will check percentage of classes, multiply it to 10^-n. Probaply this approach will not work, but I will try..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch split\n",
    "def batch_split(X,y,batch_size=64):\n",
    "    batch_indices = torch.LongTensor(np.random.permutation(X.shape[0]))\n",
    "    limit=int(X.shape[0]/batch_size)\n",
    "    batch_xs = torch.index_select(X,0,batch_indices[:limit])\n",
    "    batch_ys = torch.index_select(y,0,batch_indices[:limit])\n",
    "    return batch_xs,batch_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom loss\n",
    "def my_stupid_loss(outputs, labels,penalties):\n",
    "    batch_size = outputs.shape[0]\n",
    "    #define penalty\n",
    "    k=penalties.mul(labels).sum()\n",
    "    print ('loss:',k.numpy())\n",
    "    # compute the log of softmax values\n",
    "    outputs = F.log_softmax(outputs, dim=1) \n",
    "    mask=labels.byte()\n",
    "    # pick the values corresponding to the labels\n",
    "    outputs = outputs[mask]\n",
    "    # calculate loss\n",
    "    outputs=-((outputs.sum()+torch.log(k))/batch_size)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train function\n",
    "def fit(X_train,y_train,X_val,y_val,batch_size,pen,epoch):\n",
    "    best_val_scores=0\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    for i in range(0,epoch):\n",
    "        #split to batches\n",
    "        batch_x,batch_y=batch_split(X_train,y_train,batch_size)\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(batch_x)\n",
    "        # Compute and print loss\n",
    "        loss = my_stupid_loss(y_pred,batch_y,pen)\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        final=nn.Softmax()\n",
    "        #epoch score\n",
    "        y_pred=final(model(X_train))\n",
    "        train_scores=metrics.f1_score(y_train.data.numpy().argmax(1),y_pred.data.numpy().argmax(1),average='macro')\n",
    "        #batch train score\n",
    "        y_pred=final(model(batch_x))\n",
    "        batch_scores=metrics.f1_score(batch_y.data.numpy().argmax(1),y_pred.data.numpy().argmax(1),average='macro')\n",
    "        #valid score\n",
    "        y_pred=final(model(X_val))\n",
    "        val_scores=metrics.f1_score(y_val.data.numpy().argmax(1),y_pred.data.numpy().argmax(1),average='macro')\n",
    "        #Show res\n",
    "        print('Train F1: {:.4f} Batch F1: {:.4f}  Val F1: {:.4f}'.format(train_scores, batch_scores, val_scores))\n",
    "        #save best model\n",
    "        if best_val_scores>val_scores:\n",
    "            best_val_scores=val_scores\n",
    "            best_model=copy.deepcopy(model.state_dict())\n",
    "        model.load_state_dict(best_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset check point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=load_set('data_merge_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mean_vectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(next(iter(w2v.values())))\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anis_anton_v/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(51305, 300)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform data\n",
    "odel = word2vec.Word2Vec(df.iloc[:,0],size=300,window=3, workers=5)\n",
    "w2v = dict(zip(odel.wv.index2word, odel.wv.syn0))\n",
    "X=mean_vectorizer(w2v).fit(df.iloc[:,0]).transform(df.iloc[:,0])\n",
    "y=df.iloc[:,-1].to_numpy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot labels endcoding\n",
    "one=np.zeros((data_mean.shape[0],5))\n",
    "for i in range(0,len(y)):\n",
    "    one[i,y[i]]=1\n",
    "y=one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shapes\n",
    "X.shape[0]==y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=Variable(torch.FloatTensor(X_train))\n",
    "X_test=Variable(torch.FloatTensor(X_test))\n",
    "X_val=Variable(torch.FloatTensor(X_val))\n",
    "y_train=Variable(torch.FloatTensor(y_train))\n",
    "y_test=Variable(torch.FloatTensor(y_test))\n",
    "y_val=Variable(torch.FloatTensor(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model=mynet(X_train.shape[1],220,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance percentage\n",
    "pen=df['features'].value_counts().sort_index()/df['features'].value_counts().sum()\n",
    "pen=Variable(torch.FloatTensor(pen))\n",
    "pen*=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.1674341\n",
      "Train F1: 0.1620 Batch F1: 0.1568  Val F1: 0.1658\n",
      "loss: 0.16754822\n",
      "Train F1: 0.1620 Batch F1: 0.1417  Val F1: 0.1658\n",
      "loss: 0.16505079\n",
      "Train F1: 0.1620 Batch F1: 0.1583  Val F1: 0.1658\n",
      "loss: 0.16340503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anis_anton_v/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/anis_anton_v/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/anis_anton_v/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.1620 Batch F1: 0.1595  Val F1: 0.1658\n",
      "loss: 0.17071506\n",
      "Train F1: 0.1620 Batch F1: 0.1547  Val F1: 0.1658\n",
      "loss: 0.16672148\n",
      "Train F1: 0.1620 Batch F1: 0.1504  Val F1: 0.1658\n",
      "loss: 0.16162227\n",
      "Train F1: 0.1620 Batch F1: 0.1682  Val F1: 0.1658\n",
      "loss: 0.16442269\n",
      "Train F1: 0.1620 Batch F1: 0.1655  Val F1: 0.1658\n",
      "loss: 0.16405791\n",
      "Train F1: 0.1620 Batch F1: 0.1671  Val F1: 0.1658\n",
      "loss: 0.16456789\n",
      "Train F1: 0.1620 Batch F1: 0.1609  Val F1: 0.1658\n"
     ]
    }
   ],
   "source": [
    "model=fit(X_train,y_train,X_val,y_val,batch_size=64,pen=pen,epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       509\n",
      "           1       0.14      0.47      0.21      2237\n",
      "           2       0.66      0.57      0.61     10891\n",
      "           3       0.00      0.00      0.00      2580\n",
      "           4       0.00      0.00      0.00       714\n",
      "\n",
      "    accuracy                           0.43     16931\n",
      "   macro avg       0.16      0.21      0.16     16931\n",
      "weighted avg       0.44      0.43      0.42     16931\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anis_anton_v/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "final=nn.Softmax()\n",
    "y_pred=final(model(X_test))\n",
    "print (metrics.classification_report(y_test.data.numpy().argmax(1),y_pred.data.numpy().argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I did several experiments and all of them shown that I did a terrible(stupid and wrong) shit. Moreover, I'm sure that I used W2v wrong.\n",
    " ## So anyway lets compare result:</br>\n",
    "     1)The best model,based on assumption that test set is keggle test set and metrics f1 score, was RF tfidf and Log-reg w2v.\n",
    "    BUT it is because test set consists only of targets from 2-nd class.\n",
    "     2) In practice best F1 macro score on train set was shown by BOW Log reg.\n",
    " ## Summize, I am sure I did something wrong(maybe even all the task) ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.4,
   "position": {
    "height": "40px",
    "left": "1168px",
    "right": "20px",
    "top": "123px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
