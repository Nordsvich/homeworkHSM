{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final home assignment. Iphone or Not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zhanna Azizova**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**\n",
    " - You should to create model to detect iphone (all versions) on a picture. \n",
    " - If picture contains two or more iphones you should return only one probability for all picture. \n",
    " - Picture is a typical for internet shop. Solution should contains all ML stages (you can skip collect data stage) and pretrained model in git or link to another data storage. \n",
    " - Also you should to provide example running inference.\n",
    "\n",
    "*Interface to detection:*\n",
    "- python predict.py \n",
    "- --model path_to_model \n",
    "- --input path_to_input_data \n",
    "- --output path_to_results\n",
    "- input data - folder with images \n",
    "- output data - csv file with two columns: image_name, iphone_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import warnings\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.contrib import lite\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and model architecture preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to quickly build a good Deep neural network is to train alredy pre-trained model. One of the most powerful models is ResNet18, so thanks to Alexander's advice I have chosen this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_models.resnet import ResNet18\n",
    "\n",
    "model = ResNet18(input_shape=(224,224,3), weights='imagenet', include_top=True)\n",
    "\n",
    "model.layers.pop() #remove the last network's layer\n",
    "last_layer = model.layers[-1].output\n",
    "new_layer = Dense(2, activation=\"softmax\")(last_layer)\n",
    "model = Model(model.input, new_layer) #add a new layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model with adam optimizer\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the folder with train and test data (images)\n",
    "\n",
    "local_zip = 'data_phones.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('train_test_data')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify path for train and test folders\n",
    "\n",
    "train_data = 'train_test_data/data_phones/train'\n",
    "test_data = 'train_test_data/data_phones/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale train and test image data \n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255.)\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 59360 images belonging to 2 classes.\n",
      "Found 4084 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# define a batch size\n",
    "b_size = 32\n",
    "\n",
    "# flow train and test images using image data generator\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(train_data,\n",
    "                                              batch_size = b_size,\n",
    "                                              class_mode = 'categorical', \n",
    "                                              target_size = (224, 224))     \n",
    "\n",
    "test_gen = test_datagen.flow_from_directory(test_data,\n",
    "                                            batch_size  = b_size,\n",
    "                                            class_mode  = 'categorical', \n",
    "                                            target_size = (224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For validation set there were equally-sized samples (2024 images for each class: iphone / other_brand). However, for train set I have put approximately 35000 images of iphone class and 25000 images of other brands' phones. So the train sample was actually impalances.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and squeezing the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set checkpoint to not loose the training results\n",
    "\n",
    "model_path=\"best_model.hdf5\"\n",
    "callback = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "1855/1855 [==============================] - 978s 527ms/step - loss: 0.5894 - acc: 0.6751 - val_loss: 0.9877 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.55999, saving model to best_model.hdf5\n",
      "Epoch 2/80\n",
      "1855/1855 [==============================] - 965s 520ms/step - loss: 0.4573 - acc: 0.7764 - val_loss: 0.4191 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.55999 to 0.80730, saving model to best_model.hdf5\n",
      "Epoch 3/80\n",
      "1855/1855 [==============================] - 959s 517ms/step - loss: 0.4034 - acc: 0.8050 - val_loss: 0.4260 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.80730\n",
      "Epoch 4/80\n",
      "1855/1855 [==============================] - 966s 521ms/step - loss: 0.3731 - acc: 0.8234 - val_loss: 0.4211 - val_acc: 0.8217\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.80730 to 0.82174, saving model to best_model.hdf5\n",
      "Epoch 5/80\n",
      "1855/1855 [==============================] - 963s 519ms/step - loss: 0.3500 - acc: 0.8352 - val_loss: 0.2881 - val_acc: 0.8692\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.82174 to 0.86925, saving model to best_model.hdf5\n",
      "Epoch 6/80\n",
      "1855/1855 [==============================] - 968s 522ms/step - loss: 0.3297 - acc: 0.8458 - val_loss: 0.4079 - val_acc: 0.8166\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.86925\n",
      "Epoch 7/80\n",
      "1855/1855 [==============================] - 958s 516ms/step - loss: 0.3143 - acc: 0.8537 - val_loss: 0.2926 - val_acc: 0.8683\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.86925\n",
      "Epoch 8/80\n",
      "1855/1855 [==============================] - 960s 518ms/step - loss: 0.3004 - acc: 0.8608 - val_loss: 0.2853 - val_acc: 0.8857\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.86925 to 0.88565, saving model to best_model.hdf5\n",
      "Epoch 9/80\n",
      "1855/1855 [==============================] - 958s 516ms/step - loss: 0.2888 - acc: 0.8670 - val_loss: 0.3083 - val_acc: 0.8643\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.88565\n",
      "Epoch 10/80\n",
      "1855/1855 [==============================] - 959s 517ms/step - loss: 0.2786 - acc: 0.8717 - val_loss: 0.2832 - val_acc: 0.8817\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.88565\n",
      "Epoch 11/80\n",
      "1855/1855 [==============================] - 969s 522ms/step - loss: 0.2740 - acc: 0.8734 - val_loss: 0.2482 - val_acc: 0.8957\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.88565 to 0.89569, saving model to best_model.hdf5\n",
      "Epoch 12/80\n",
      "1855/1855 [==============================] - 963s 519ms/step - loss: 0.2627 - acc: 0.8809 - val_loss: 0.2158 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.89569 to 0.90573, saving model to best_model.hdf5\n",
      "Epoch 13/80\n",
      "1855/1855 [==============================] - 970s 523ms/step - loss: 0.2529 - acc: 0.8856 - val_loss: 0.2487 - val_acc: 0.8940\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.90573\n",
      "Epoch 14/80\n",
      "1855/1855 [==============================] - 962s 519ms/step - loss: 0.2503 - acc: 0.8870 - val_loss: 0.2919 - val_acc: 0.8869\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.90573\n",
      "Epoch 15/80\n",
      "1855/1855 [==============================] - 962s 519ms/step - loss: 0.2398 - acc: 0.8919 - val_loss: 0.2481 - val_acc: 0.8937\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.90573\n",
      "Epoch 16/80\n",
      "1855/1855 [==============================] - 963s 519ms/step - loss: 0.2364 - acc: 0.8939 - val_loss: 0.2327 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.90573\n",
      "Epoch 17/80\n",
      "1855/1855 [==============================] - 969s 522ms/step - loss: 0.2301 - acc: 0.8974 - val_loss: 0.2061 - val_acc: 0.9153\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.90573 to 0.91528, saving model to best_model.hdf5\n",
      "Epoch 18/80\n",
      "1855/1855 [==============================] - 956s 516ms/step - loss: 0.2233 - acc: 0.9002 - val_loss: 0.2294 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.91528\n",
      "Epoch 19/80\n",
      "1855/1855 [==============================] - 968s 522ms/step - loss: 0.2205 - acc: 0.9011 - val_loss: 0.2297 - val_acc: 0.9048\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.91528\n",
      "Epoch 20/80\n",
      "1855/1855 [==============================] - 967s 521ms/step - loss: 0.2208 - acc: 0.9009 - val_loss: 0.2117 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.91528 to 0.91577, saving model to best_model.hdf5\n",
      "Epoch 21/80\n",
      "1855/1855 [==============================] - 958s 517ms/step - loss: 0.2161 - acc: 0.9030 - val_loss: 0.2371 - val_acc: 0.8979\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.91577\n",
      "Epoch 22/80\n",
      "1855/1855 [==============================] - 959s 517ms/step - loss: 0.2100 - acc: 0.9059 - val_loss: 0.2000 - val_acc: 0.9155\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.91577\n",
      "Epoch 23/80\n",
      "1855/1855 [==============================] - 967s 521ms/step - loss: 0.2074 - acc: 0.9070 - val_loss: 0.2493 - val_acc: 0.8981\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.91577\n",
      "Epoch 24/80\n",
      "1855/1855 [==============================] - 961s 518ms/step - loss: 0.2042 - acc: 0.9083 - val_loss: 0.2058 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.91577\n",
      "Epoch 25/80\n",
      "1855/1855 [==============================] - 959s 517ms/step - loss: 0.1989 - acc: 0.9108 - val_loss: 0.1795 - val_acc: 0.9229\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.91577 to 0.92287, saving model to best_model.hdf5\n",
      "Epoch 26/80\n",
      "1855/1855 [==============================] - 969s 522ms/step - loss: 0.1956 - acc: 0.9126 - val_loss: 0.3761 - val_acc: 0.8528\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.92287\n",
      "Epoch 27/80\n",
      "1855/1855 [==============================] - 959s 517ms/step - loss: 0.1942 - acc: 0.9150 - val_loss: 0.2274 - val_acc: 0.9099\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.92287\n",
      "Epoch 28/80\n",
      "1855/1855 [==============================] - 960s 518ms/step - loss: 0.1936 - acc: 0.9131 - val_loss: 0.1705 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.92287 to 0.93291, saving model to best_model.hdf5\n",
      "Epoch 29/80\n",
      "1855/1855 [==============================] - 962s 519ms/step - loss: 0.1887 - acc: 0.9157 - val_loss: 0.1868 - val_acc: 0.9251\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.93291\n",
      "Epoch 30/80\n",
      "1855/1855 [==============================] - 964s 520ms/step - loss: 0.1873 - acc: 0.9169 - val_loss: 0.2131 - val_acc: 0.9126\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.93291\n",
      "Epoch 31/80\n",
      "1855/1855 [==============================] - 962s 518ms/step - loss: 0.1865 - acc: 0.9177 - val_loss: 0.1983 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.93291\n",
      "Epoch 32/80\n",
      "1855/1855 [==============================] - 963s 519ms/step - loss: 0.1812 - acc: 0.9198 - val_loss: 0.2258 - val_acc: 0.9116\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.93291\n",
      "Epoch 33/80\n",
      "1855/1855 [==============================] - 961s 518ms/step - loss: 0.1785 - acc: 0.9211 - val_loss: 0.1597 - val_acc: 0.9349\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.93291 to 0.93487, saving model to best_model.hdf5\n",
      "Epoch 34/80\n",
      "1855/1855 [==============================] - 959s 517ms/step - loss: 0.1772 - acc: 0.9213 - val_loss: 0.1691 - val_acc: 0.9322\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.93487\n",
      "Epoch 35/80\n",
      "1855/1855 [==============================] - 965s 520ms/step - loss: 0.1778 - acc: 0.9214 - val_loss: 0.1717 - val_acc: 0.9356\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.93487 to 0.93560, saving model to best_model.hdf5\n",
      "Epoch 36/80\n",
      "1855/1855 [==============================] - 963s 519ms/step - loss: 0.1754 - acc: 0.9227 - val_loss: 0.2723 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.93560\n",
      "Epoch 37/80\n",
      "1855/1855 [==============================] - 958s 517ms/step - loss: 0.1736 - acc: 0.9255 - val_loss: 0.1703 - val_acc: 0.9317\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.93560\n",
      "Epoch 38/80\n",
      "1855/1855 [==============================] - 967s 521ms/step - loss: 0.1571 - acc: 0.9318 - val_loss: 0.1881 - val_acc: 0.9212\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.94221\n",
      "Epoch 48/80\n",
      "1855/1855 [==============================] - 963s 519ms/step - loss: 0.1532 - acc: 0.9343 - val_loss: 0.1497 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.94221\n",
      "Epoch 49/80\n",
      "1855/1855 [==============================] - 968s 522ms/step - loss: 0.1534 - acc: 0.9330 - val_loss: 0.1976 - val_acc: 0.9327\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.94221\n",
      "Epoch 50/80\n",
      "1855/1855 [==============================] - 965s 520ms/step - loss: 0.1498 - acc: 0.9349 - val_loss: 0.1891 - val_acc: 0.9292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00050: val_acc did not improve from 0.94221\n",
      "Epoch 51/80\n",
      "1855/1855 [==============================] - 981s 529ms/step - loss: 0.1521 - acc: 0.9339 - val_loss: 0.1552 - val_acc: 0.9383\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.94221\n",
      "Epoch 52/80\n",
      "1855/1855 [==============================] - 982s 529ms/step - loss: 0.1507 - acc: 0.9345 - val_loss: 0.1568 - val_acc: 0.9434\n",
      "\n",
      "Epoch 00052: val_acc improved from 0.94221 to 0.94344, saving model to best_model.hdf5\n",
      "Epoch 53/80\n",
      "1855/1855 [==============================] - 980s 529ms/step - loss: 0.1476 - acc: 0.9361 - val_loss: 0.1506 - val_acc: 0.9425\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.94344\n",
      "Epoch 54/80\n",
      "1855/1855 [==============================] - 981s 529ms/step - loss: 0.1466 - acc: 0.9363 - val_loss: 0.1535 - val_acc: 0.9420\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.94344\n",
      "Epoch 55/80\n",
      "1855/1855 [==============================] - 988s 533ms/step - loss: 0.1458 - acc: 0.9365 - val_loss: 0.1623 - val_acc: 0.9332\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.94344\n",
      "Epoch 56/80\n",
      "1855/1855 [==============================] - 981s 529ms/step - loss: 0.1468 - acc: 0.9363 - val_loss: 0.1525 - val_acc: 0.9434\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.94344\n",
      "Epoch 57/80\n",
      "1855/1855 [==============================] - 967s 521ms/step - loss: 0.1436 - acc: 0.9380 - val_loss: 0.1896 - val_acc: 0.9280\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.94344\n",
      "Epoch 58/80\n",
      "1855/1855 [==============================] - 965s 520ms/step - loss: 0.1446 - acc: 0.9367 - val_loss: 0.1608 - val_acc: 0.9417\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.94344\n",
      "Epoch 59/80\n",
      "1855/1855 [==============================] - 962s 518ms/step - loss: 0.1421 - acc: 0.9378 - val_loss: 0.1489 - val_acc: 0.9432\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.94344\n",
      "Epoch 60/80\n",
      "1855/1855 [==============================] - 962s 519ms/step - loss: 0.1414 - acc: 0.9396 - val_loss: 0.1680 - val_acc: 0.9339\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.94344\n",
      "Epoch 61/80\n",
      "1855/1855 [==============================] - 972s 524ms/step - loss: 0.1389 - acc: 0.9407 - val_loss: 0.1434 - val_acc: 0.9437\n",
      "\n",
      "Epoch 00061: val_acc improved from 0.94344 to 0.94368, saving model to best_model.hdf5\n",
      "Epoch 62/80\n",
      "1855/1855 [==============================] - 964s 520ms/step - loss: 0.1370 - acc: 0.9407 - val_loss: 0.2643 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.94368\n",
      "Epoch 63/80\n",
      "1855/1855 [==============================] - 963s 519ms/step - loss: 0.1379 - acc: 0.9407 - val_loss: 0.1530 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.94368\n",
      "Epoch 64/80\n",
      "1855/1855 [==============================] - 966s 521ms/step - loss: 0.1351 - acc: 0.9424 - val_loss: 0.2026 - val_acc: 0.9243\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.94368\n",
      "Epoch 65/80\n",
      "1855/1855 [==============================] - 970s 523ms/step - loss: 0.1360 - acc: 0.9420 - val_loss: 0.1605 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.94368\n",
      "Epoch 66/80\n",
      "1855/1855 [==============================] - 975s 526ms/step - loss: 0.1366 - acc: 0.9410 - val_loss: 0.2141 - val_acc: 0.9224\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.94368\n",
      "Epoch 67/80\n",
      "1855/1855 [==============================] - 970s 523ms/step - loss: 0.1317 - acc: 0.9429 - val_loss: 0.1719 - val_acc: 0.9336\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.94368\n",
      "Epoch 68/80\n",
      "1855/1855 [==============================] - 976s 526ms/step - loss: 0.1320 - acc: 0.9438 - val_loss: 0.1453 - val_acc: 0.9466\n",
      "\n",
      "Epoch 00068: val_acc improved from 0.94368 to 0.94662, saving model to best_model.hdf5\n",
      "Epoch 69/80\n",
      "1855/1855 [==============================] - 976s 526ms/step - loss: 0.1337 - acc: 0.9429 - val_loss: 0.1692 - val_acc: 0.9373\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.94662\n",
      "Epoch 70/80\n",
      "1855/1855 [==============================] - 984s 530ms/step - loss: 0.1306 - acc: 0.9443 - val_loss: 0.2380 - val_acc: 0.9180\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.94662\n",
      "Epoch 71/80\n",
      "1855/1855 [==============================] - 973s 525ms/step - loss: 0.1315 - acc: 0.9439 - val_loss: 0.1624 - val_acc: 0.9371\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.94662\n",
      "Epoch 72/80\n",
      "1855/1855 [==============================] - 973s 525ms/step - loss: 0.1277 - acc: 0.9457 - val_loss: 0.1493 - val_acc: 0.9429\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.94662\n",
      "Epoch 73/80\n",
      "1855/1855 [==============================] - 974s 525ms/step - loss: 0.1288 - acc: 0.9448 - val_loss: 0.1441 - val_acc: 0.9459\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.94662\n",
      "Epoch 74/80\n",
      "1855/1855 [==============================] - 975s 526ms/step - loss: 0.1274 - acc: 0.9460 - val_loss: 0.1375 - val_acc: 0.9461\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.94662\n",
      "Epoch 75/80\n",
      "1855/1855 [==============================] - 975s 526ms/step - loss: 0.1263 - acc: 0.9456 - val_loss: 0.1826 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.94662\n",
      "Epoch 76/80\n",
      "1855/1855 [==============================] - 977s 527ms/step - loss: 0.1254 - acc: 0.9464 - val_loss: 0.1655 - val_acc: 0.9332\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.94662\n",
      "Epoch 77/80\n",
      "1855/1855 [==============================] - 978s 527ms/step - loss: 0.1242 - acc: 0.9468 - val_loss: 0.1536 - val_acc: 0.9456\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.94662\n",
      "Epoch 78/80\n",
      "1855/1855 [==============================] - 968s 522ms/step - loss: 0.1251 - acc: 0.9461 - val_loss: 0.4558 - val_acc: 0.8754\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.94662\n",
      "Epoch 79/80\n",
      "1855/1855 [==============================] - 980s 528ms/step - loss: 0.1224 - acc: 0.9472 - val_loss: 0.1395 - val_acc: 0.9493\n",
      "\n",
      "Epoch 00079: val_acc improved from 0.94662 to 0.94931, saving model to best_model.hdf5\n",
      "Epoch 80/80\n",
      "1855/1855 [==============================] - 977s 526ms/step - loss: 0.1205 - acc: 0.9489 - val_loss: 0.1369 - val_acc: 0.9510\n",
      "\n",
      "Epoch 00080: val_acc improved from 0.94931 to 0.95103, saving model to best_model.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0422fff28>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model on batches (of 32 images) with real-time data augmentation\n",
    "\n",
    "model.fit_generator(train_gen,\n",
    "                    validation_data = test_gen,\n",
    "                    steps_per_epoch = train_gen.samples / b_size,\n",
    "                    epochs = 80,\n",
    "                    validation_steps = test_gen.samples / b_size,\n",
    "                    verbose = 1,\n",
    "                    callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** As we can see the best model reached 95% accuracy on the validation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /jet/var/python/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /jet/var/python/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /jet/var/python/lib/python3.6/site-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /jet/var/python/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 100 variables.\n",
      "INFO:tensorflow:Converted 100 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46791104"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_converter = lite.TFLiteConverter.from_keras_model_file('best_model.hdf5')\n",
    "squeezed_model = tflite_converter.convert()\n",
    "open(\"phone_classification_model.tflite\", \"wb\").write(squeezed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading and rescaling images\n",
    "\n",
    "def prepare_image(path):\n",
    "    \n",
    "    loaded_image = image.load_img(path, target_size=(224, 224))\n",
    "    processed_image = image.img_to_array(loaded_image)\n",
    "    processed_image = np.expand_dims(processed_image, axis=0)\n",
    "    processed_image /= 255.\n",
    "    \n",
    "    return processed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predicting(model_path, input_folder):\n",
    "\n",
    "    model = tf.lite.Interpreter(model_path=model_path)\n",
    "    model.allocate_tensors()\n",
    "    input_tensors = model.get_input_details()\n",
    "    output_tensors = model.get_output_details()\n",
    "\n",
    "    probas = []\n",
    "    preds = []\n",
    "    classes = []\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(input_folder):\n",
    "        for i, file in enumerate(files):\n",
    "            input_data = prepare_image(os.path.join(subdir, file))\n",
    "            model.set_tensor(input_tensors[0]['index'], input_data)\n",
    "            model.invoke()\n",
    "            probability = model.get_tensor(output_tensors[0]['index'])\n",
    "            \n",
    "            probas.append(probability[0][0])\n",
    "            \n",
    "            classes.append(os.path.basename(subdir))\n",
    "            \n",
    "            if probability[0][0] > 0.5:\n",
    "                preds.append(1)\n",
    "            else:\n",
    "                preds.append(0)\n",
    "            \n",
    "    return classes,probas, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl, prob, pred = run_predicting('phone_classification_model.tflite', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = {'iphone':1,'other_brands':0}\n",
    "y_true = [labs[x] if x in labs else 'none' for x in cl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.9919196965339646\n",
      "Precision score: 0.9513440375074876\n",
      "Recall score: 0.951028403525955\n",
      "F1 score: 0.9510198403227605\n",
      "ROC AUC score: 0.9911210802337974\n"
     ]
    }
   ],
   "source": [
    "print(\"Average precision score: {}\".format(average_precision_score(y_true, prob)))\n",
    "print(\"Precision score: {}\".format(precision_score(y_true, pred, average = 'macro')))\n",
    "print(\"Recall score: {}\".format(recall_score(y_true, pred, average = 'macro')))\n",
    "print(\"F1 score: {}\".format(f1_score(y_true, pred, average='macro')))\n",
    "print(\"ROC AUC score: {}\".format(roc_auc_score(y_true, prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1915,  127],\n",
       "       [  73, 1969]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Running inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After model is saved, I have created a predict.py file with arguments required and launch the comand below in the terminal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "python3 predict.py  --model \"phone_classification_model.tflite\" --input \"train_test_data/data_phones/test\" --output \"predictions.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**After script running was finished, I got the csv file \"predictions.csv\" with probabilities for images in the test data to contain an iphone picture. Each probability is assigned to image file name**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
